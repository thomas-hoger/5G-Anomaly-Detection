{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5493ab29-7df9-4178-a629-19cd5df77310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structure générale:\n",
      "Nombre de batches: 54791\n",
      "Total subgraphs: 54791\n",
      "\n",
      "Batch 0 (packet_id=0):\n",
      "  Subgraphs: 1\n",
      "  Premier subgraph: 6 noeuds, 5 aretes\n",
      "\n",
      "Batch 100 (packet_id=136):\n",
      "  Subgraphs: 1\n",
      "  Premier subgraph: 6 noeuds, 5 aretes\n",
      "\n",
      "Batch 1000 (packet_id=1446):\n",
      "  Subgraphs: 1\n",
      "  Premier subgraph: 5 noeuds, 4 aretes\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Charger et inspecter\n",
    "with open('../data/05_model_input/gnn_training/training_data_main.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "print(\"Structure générale:\")\n",
    "print(f\"Nombre de batches: {len(data['batches'])}\")\n",
    "print(f\"Total subgraphs: {data['stats']['total_subgraphs']}\")\n",
    "\n",
    "# Examiner quelques batches\n",
    "for i in [0, 100, 1000]:  # Début, milieu, plus loin\n",
    "    if i < len(data['batches']):\n",
    "        batch = data['batches'][i]\n",
    "        print(f\"\\nBatch {i} (packet_id={batch['packet_id']}):\")\n",
    "        print(f\"  Subgraphs: {batch['count']}\")\n",
    "        \n",
    "        # Premier subgraph du batch\n",
    "        if batch['subgraphs']:\n",
    "            sg = batch['subgraphs'][0]\n",
    "            print(f\"  Premier subgraph: {sg['x'].shape[0]} noeuds, {sg['edge_index'].shape[1]} aretes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20badd8a-c0e2-428d-bf01-d191512219bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packet_IDs progression:\n",
      "Min: 0, Max: 80999\n",
      "Ordre croissant: True\n",
      "Premiers: [0, 1, 2, 3, 4, 6, 7, 8, 9, 10]\n",
      "Derniers: [80985, 80986, 80988, 80990, 80992, 80993, 80995, 80996, 80998, 80999]\n"
     ]
    }
   ],
   "source": [
    "# Vérifier ordre packet_ids\n",
    "packet_ids = [batch['packet_id'] for batch in data['batches']]\n",
    "print(\"Packet_IDs progression:\")\n",
    "print(f\"Min: {min(packet_ids)}, Max: {max(packet_ids)}\")\n",
    "print(f\"Ordre croissant: {packet_ids == sorted(packet_ids)}\")\n",
    "print(f\"Premiers: {packet_ids[:10]}\")\n",
    "print(f\"Derniers: {packet_ids[-10:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f55cbb42-2465-42ee-a13f-34e2891549d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subgraphs par batch: min=1, max=1, moy=1.0\n",
      "Nœuds par subgraph: min=5, max=30, moy=7.9\n"
     ]
    }
   ],
   "source": [
    "# Analyser distribution tailles\n",
    "subgraph_counts = [batch['count'] for batch in data['batches']]\n",
    "node_counts = []\n",
    "\n",
    "for batch in data['batches'][:100]:  # Échantillon\n",
    "    for sg in batch['subgraphs']:\n",
    "        node_counts.append(sg['x'].shape[0])\n",
    "\n",
    "print(f\"Subgraphs par batch: min={min(subgraph_counts)}, max={max(subgraph_counts)}, moy={sum(subgraph_counts)/len(subgraph_counts):.1f}\")\n",
    "print(f\"Nœuds par subgraph: min={min(node_counts)}, max={max(node_counts)}, moy={sum(node_counts)/len(node_counts):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22e4f9f8-6cdc-44c4-8fd6-1eb43dd57054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test mini-entraînement:\n",
      "Packet_ID 0: 1 subgraphs\n",
      "  Subgraph 0: nodes=torch.Size([6, 64]), edges=torch.Size([2, 5]), edge_attr=torch.Size([5, 64])\n",
      "    Test loss: 0.0000 - OK\n",
      "Packet_ID 1: 1 subgraphs\n",
      "  Subgraph 0: nodes=torch.Size([5, 64]), edges=torch.Size([2, 4]), edge_attr=torch.Size([4, 64])\n",
      "    Test loss: 0.0000 - OK\n",
      "Packet_ID 2: 1 subgraphs\n",
      "  Subgraph 0: nodes=torch.Size([10, 64]), edges=torch.Size([2, 597]), edge_attr=torch.Size([597, 64])\n",
      "    Test loss: 0.0000 - OK\n",
      "Packet_ID 3: 1 subgraphs\n",
      "  Subgraph 0: nodes=torch.Size([12, 64]), edges=torch.Size([2, 14]), edge_attr=torch.Size([14, 64])\n",
      "    Test loss: 0.0000 - OK\n",
      "Packet_ID 4: 1 subgraphs\n",
      "  Subgraph 0: nodes=torch.Size([5, 64]), edges=torch.Size([2, 4]), edge_attr=torch.Size([4, 64])\n",
      "    Test loss: 0.0000 - OK\n",
      "Packet_ID 6: 1 subgraphs\n",
      "  Subgraph 0: nodes=torch.Size([6, 64]), edges=torch.Size([2, 5]), edge_attr=torch.Size([5, 64])\n",
      "    Test loss: 0.0000 - OK\n",
      "Packet_ID 7: 1 subgraphs\n",
      "  Subgraph 0: nodes=torch.Size([5, 64]), edges=torch.Size([2, 4]), edge_attr=torch.Size([4, 64])\n",
      "    Test loss: 0.0000 - OK\n",
      "Packet_ID 8: 1 subgraphs\n",
      "  Subgraph 0: nodes=torch.Size([16, 64]), edges=torch.Size([2, 803]), edge_attr=torch.Size([803, 64])\n",
      "    Test loss: 0.0000 - OK\n",
      "Packet_ID 9: 1 subgraphs\n",
      "  Subgraph 0: nodes=torch.Size([10, 64]), edges=torch.Size([2, 9]), edge_attr=torch.Size([9, 64])\n",
      "    Test loss: 0.0000 - OK\n",
      "Packet_ID 10: 1 subgraphs\n",
      "  Subgraph 0: nodes=torch.Size([6, 64]), edges=torch.Size([2, 9]), edge_attr=torch.Size([9, 64])\n",
      "    Test loss: 0.0000 - OK\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Test sur les 10 premiers batches\n",
    "print(\"Test mini-entraînement:\")\n",
    "\n",
    "for i, batch in enumerate(data['batches'][:10]):\n",
    "    packet_id = batch['packet_id']\n",
    "    subgraphs = batch['subgraphs']\n",
    "    \n",
    "    print(f\"Packet_ID {packet_id}: {len(subgraphs)} subgraphs\")\n",
    "    \n",
    "    # Test chaque subgraph\n",
    "    for j, sg in enumerate(subgraphs):\n",
    "        x = sg['x']  # Node features\n",
    "        edge_index = sg['edge_index']  # Connections\n",
    "        edge_attr = sg['edge_attr']  # Edge features\n",
    "        \n",
    "        print(f\"  Subgraph {j}: nodes={x.shape}, edges={edge_index.shape}, edge_attr={edge_attr.shape}\")\n",
    "        \n",
    "        # Test basique: peut-on calculer une loss simple ?\n",
    "        try:\n",
    "            # Reconstruction fictive (identité)\n",
    "            x_reconstructed = x.clone()\n",
    "            \n",
    "            # Loss MSE\n",
    "            loss = F.mse_loss(x_reconstructed, x)\n",
    "            print(f\"    Test loss: {loss.item():.4f} - OK\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    ERREUR: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb3e742a-9f16-4b0f-bdfc-e26aaa312903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset extrait: 54791 graphiques PyTorch\n",
      "Sample: x=torch.Size([6, 64]), edge_index=torch.Size([2, 5])\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import torch\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "\n",
    "# Charger vos données DÉJÀ transformées\n",
    "with open('../data/05_model_input/gnn_training/training_data_main.pkl', 'rb') as f:\n",
    "    training_data = pickle.load(f)\n",
    "\n",
    "def extract_pytorch_graphs(training_data):\n",
    "    \"\"\"Extraire les graphiques PyTorch déjà convertis\"\"\"\n",
    "    pytorch_graphs = []\n",
    "    \n",
    "    for batch in training_data['batches']:\n",
    "        for subgraph_data in batch['subgraphs']:\n",
    "            # Les données sont DÉJÀ en format PyTorch !\n",
    "            data = Data(\n",
    "                x=subgraph_data['x'],                    # Déjà torch.Tensor\n",
    "                edge_index=subgraph_data['edge_index'],  # Déjà torch.Tensor  \n",
    "                edge_attr=subgraph_data['edge_attr'],    # Déjà torch.Tensor\n",
    "                packet_id=batch['packet_id'],\n",
    "                synthetic_edge=subgraph_data['synthetic_edge']\n",
    "            )\n",
    "            pytorch_graphs.append(data)\n",
    "    \n",
    "    return pytorch_graphs\n",
    "\n",
    "# Extraction immédiate\n",
    "main_dataset = extract_pytorch_graphs(training_data)\n",
    "print(f\"Dataset extrait: {len(main_dataset)} graphiques PyTorch\")\n",
    "\n",
    "# Vérification du format\n",
    "sample = main_dataset[0]\n",
    "print(f\"Sample: x={sample.x.shape}, edge_index={sample.edge_index.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c92abbf9-531d-40a0-8647-1fb262463b68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ANALYSE DES DONNÉES (DICTIONNAIRE) ===\n",
      "Type: <class 'dict'>\n",
      "Clés du dictionnaire: ['batches', 'stats', 'status']\n",
      "Nombre de clés: 3\n",
      "\n",
      "Clé 'batches':\n",
      "  Type: <class 'list'>\n",
      "  Taille liste: 54791\n",
      "  Premier élément de la liste: <class 'dict'>\n",
      "\n",
      "Clé 'stats':\n",
      "  Type: <class 'dict'>\n",
      "  Contenu: {'total_batches': 54791, 'total_subgraphs': 54791, 'avg_subgraphs_per_batch': 1.0}\n",
      "\n",
      "Clé 'status':\n",
      "  Type: <class 'str'>\n",
      "  Contenu: READY\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Charger le fichier\n",
    "with open(\"../data/05_model_input/gnn_training/training_data_main.pkl\", 'rb') as f:\n",
    "    training_data = pickle.load(f)\n",
    "\n",
    "print(\"=== ANALYSE DES DONNÉES (DICTIONNAIRE) ===\")\n",
    "\n",
    "# Type et contenu\n",
    "print(f\"Type: {type(training_data)}\")\n",
    "\n",
    "if isinstance(training_data, dict):\n",
    "    print(f\"Clés du dictionnaire: {list(training_data.keys())}\")\n",
    "    print(f\"Nombre de clés: {len(training_data.keys())}\")\n",
    "    \n",
    "    # Analyser chaque clé\n",
    "    for key in list(training_data.keys())[:3]:  # Premiers 3 seulement\n",
    "        value = training_data[key]\n",
    "        print(f\"\\nClé '{key}':\")\n",
    "        print(f\"  Type: {type(value)}\")\n",
    "        \n",
    "        if isinstance(value, list):\n",
    "            print(f\"  Taille liste: {len(value)}\")\n",
    "            if len(value) > 0:\n",
    "                print(f\"  Premier élément de la liste: {type(value[0])}\")\n",
    "                if hasattr(value[0], 'x'):\n",
    "                    print(f\"    Subgraph - Nodes: {value[0].x.shape}, Edges: {value[0].edge_index.shape}\")\n",
    "        \n",
    "        elif hasattr(value, 'x'):\n",
    "            print(f\"  Subgraph direct - Nodes: {value.x.shape}, Edges: {value.edge_index.shape}\")\n",
    "        \n",
    "        else:\n",
    "            print(f\"  Contenu: {value}\")\n",
    "\n",
    "else:\n",
    "    print(\"Ce n'est pas un dictionnaire...\")\n",
    "    print(f\"Type réel: {type(training_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d52e678-117b-484d-af54-d2f68e303131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premier batch: <class 'dict'>\n",
      "Clés du premier batch: dict_keys(['packet_id', 'subgraphs', 'count'])\n",
      "  packet_id: <class 'int'>\n",
      "  subgraphs: <class 'list'>\n",
      "    Premier élément: <class 'dict'>\n",
      "  count: <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "# Dans votre notebook\n",
    "batches = training_data['batches']\n",
    "print(f\"Premier batch: {type(batches[0])}\")\n",
    "print(f\"Clés du premier batch: {batches[0].keys()}\")\n",
    "\n",
    "# Analyser la structure d'un batch\n",
    "first_batch = batches[0]\n",
    "for key, value in first_batch.items():\n",
    "    print(f\"  {key}: {type(value)}\")\n",
    "    if isinstance(value, list) and len(value) > 0:\n",
    "        print(f\"    Premier élément: {type(value[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d77a9d0b-35c6-4146-9f0c-086dfe11494c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ANALYSE DU SUBGRAPH ===\n",
      "Type du subgraph: <class 'dict'>\n",
      "Clés: dict_keys(['x', 'edge_index', 'edge_attr', 'num_nodes', 'num_edges', 'synthetic_edge'])\n",
      "  x: <class 'torch.Tensor'>\n",
      "    Shape: torch.Size([6, 64])\n",
      "  edge_index: <class 'torch.Tensor'>\n",
      "    Shape: torch.Size([2, 5])\n",
      "  edge_attr: <class 'torch.Tensor'>\n",
      "    Shape: torch.Size([5, 64])\n",
      "  num_nodes: <class 'int'>\n",
      "  num_edges: <class 'int'>\n",
      "  synthetic_edge: <class 'bool'>\n"
     ]
    }
   ],
   "source": [
    "# Dans votre notebook\n",
    "batches = training_data['batches']\n",
    "first_batch = batches[0]\n",
    "first_subgraph = first_batch['subgraphs'][0]\n",
    "\n",
    "print(\"=== ANALYSE DU SUBGRAPH ===\")\n",
    "print(f\"Type du subgraph: {type(first_subgraph)}\")\n",
    "print(f\"Clés: {first_subgraph.keys() if isinstance(first_subgraph, dict) else 'Pas un dict'}\")\n",
    "\n",
    "# Si c'est un dict, analyser le contenu\n",
    "if isinstance(first_subgraph, dict):\n",
    "    for key, value in first_subgraph.items():\n",
    "        print(f\"  {key}: {type(value)}\")\n",
    "        if hasattr(value, 'shape'):\n",
    "            print(f\"    Shape: {value.shape}\")\n",
    "        elif isinstance(value, (list, tuple)):\n",
    "            print(f\"    Longueur: {len(value)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7875ef0-f999-463f-b169-b50c5525ec0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

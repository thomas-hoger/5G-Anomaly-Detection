
#  ____  ____  _____   ____  ____   ___   ____ _____ ____ ____  
# |  _ \|  _ \| ____| |  _ \|  _ \ / _ \ / ___| ____/ ___/ ___| 
# | |_) | |_) |  _|   | |_) | |_) | | | | |   |  _| \___ \___ \ 
# |  __/|  _ <| |___  |  __/|  _ <| |_| | |___| |___ ___) |__) |
# |_|   |_| \_\_____| |_|   |_| \_\\___/ \____|_____|____/____/ 

# ==== 1) trace_cleaning_labelling

# -> Input
initial_raw_file:
  type: partitions.PartitionedDataset
  path: data/initial_raw
  dataset:
    type: networkanomalydetection.datasets.scapy_pcap_dataset.ScapyPcapDataset

# <- Output
trace_clean:
  type: partitions.PartitionedDataset
  path: data/pre_process/trace_clean
  dataset:
    type: networkanomalydetection.datasets.scapy_pcap_dataset.ScapyPcapDataset

trace_labels:
  type: partitions.PartitionedDataset
  path: data/pre_process/trace_labels
  dataset:
    type: pandas.CSVDataset

# ==== 2) trace_dissection

# <- Input
trace_to_dissect:
  type: partitions.PartitionedDataset
  path: data/pre_process/trace_clean
  dataset:
    type: networkanomalydetection.datasets.pyshark_pcap_dataset.PySharkPcapDataset

# <- Output
trace_dissected:
  type: partitions.PartitionedDataset
  path: data/pre_process/trace_dissection
  dataset:
    type: json.JSONDataset

# ==== 5) feature_split_type

# <- Output
feature_words:
  type: partitions.PartitionedDataset
  path: data/pre_process/vocabulary/words
  dataset:
    type: json.JSONDataset

feature_floats:
  type: partitions.PartitionedDataset
  path: data/pre_process/vocabulary/floats
  dataset:
    type: json.JSONDataset

# ==== 2) dissection_clean

# <- Output
dissected_clean:
  type: partitions.PartitionedDataset
  path: data/pre_process/dissection_clean
  dataset:
    type: json.JSONDataset

# ==== 2) dissection_clean

# <- Output
dissection_clusteried:
  type: partitions.PartitionedDataset
  path: data/pre_process/dissection_clusteried
  dataset:
    type: json.JSONDataset

# ==== 3) graph_construction

# <- Output
initial_graph:
  type: partitions.PartitionedDataset
  path: data/pre_process/graph_construction
  dataset:
    type: pickle.PickleDataset  

# ==== 4) graph_visualization

# <- Output
initial_graph_display:
  type: partitions.PartitionedDataset
  path: data/report/initial_graph_display
  dataset:
    type: text.TextDataset 

# ==== 6) feature_vectorization

# <- Output
vectorized_features:
  type: partitions.PartitionedDataset
  path: data/pre_process/feature_vectorization
  dataset:
    type: pickle.PickleDataset 

feature_vectorization_report:
  type: json.JSONDataset
  filepath: data/report/feature_vectorization.json

# ==== 7) graph_sampling

# <- Output
subgraphs:
  type: partitions.PartitionedDataset
  path: data/pre_process/graph_sampling
  dataset:
    type: pickle.PickleDataset 

sampling_report:
  type: json.JSONDataset
  filepath: data/report/sampling.json

# ==== 7) graph_conversion

# <- Output
data_loader_1:
  type: pickle.PickleDataset 
  filepath: data/pre_process/data_loaders/loader_1.pkl

data_loader_2:
  type: pickle.PickleDataset 
  filepath: data/pre_process/data_loaders/loader_2.pkl


# === CONVERSION GNN : Pipeline Vectorisation ===
gnn_pytorch_data:
  type: pickle.PickleDataset
  filepath: data/05_model_input/gnn_pytorch_data.pkl

conversion_metadata:
  type: pickle.PickleDataset
  filepath: data/reporting/conversion_metadata.pkl

conversion_validation_report:
  type: json.JSONDataset
  filepath: data/reporting/conversion_validation_report.json

conversion_quality_report:
  type: json.JSONDataset
  filepath: data/reporting/conversion_quality_report.json

# === CONVERSION GNN - SYSTÈME BASELINE ===
baseline_gnn_pytorch_data:
  type: pickle.PickleDataset
  filepath: data/05_model_input/baseline_gnn_pytorch_data.pkl

baseline_conversion_metadata:
  type: pickle.PickleDataset
  filepath: data/reporting/baseline_conversion_metadata.pkl

baseline_conversion_validation_report:
  type: json.JSONDataset
  filepath: data/reporting/baseline_conversion_validation_report.json

baseline_conversion_quality_report:
  type: json.JSONDataset
  filepath: data/reporting/baseline_conversion_quality_report.json

# Configuration catalog pour gnn_sampling
# Ajouter dans conf/base/catalog.yml

# Données intermédiaires
subgraphs_main_raw:
  type: pickle.PickleDataset
  filepath: data/05_model_input/gnn_sampling/subgraphs_main_raw.pkl

subgraphs_baseline_raw:
  type: pickle.PickleDataset
  filepath: data/05_model_input/gnn_sampling/subgraphs_baseline_raw.pkl

# Données finales pour entraînement
training_data_main:
  type: pickle.PickleDataset
  filepath: data/05_model_input/gnn_training/training_data_main.pkl

training_data_baseline:
  type: pickle.PickleDataset
  filepath: data/05_model_input/gnn_training/training_data_baseline.pkl

# MAIN
train_val_split_main:
  type: pickle.PickleDataset
  filepath: data/05_model_input/gnn_training/train_val_split_main.pkl

gnn_training_results_main:
  type: pickle.PickleDataset
  filepath: data/models/gnn_training_results_main.pkl

# BASELINE  
train_val_split_baseline:
  type: pickle.PickleDataset
  filepath: data/05_model_input/gnn_training/train_val_split_baseline.pkl

gnn_training_results_baseline:
  type: pickle.PickleDataset
  filepath: data/models/gnn_training_results_baseline.pkl

# COMPARAISON
model_comparison:
  type: pickle.PickleDataset
  filepath: data/reporting/gnn/model_comparison.pkl

# Graphiques GNN entraînement
training_curves_fig:
  type: matplotlib.MatplotlibWriter
  filepath: data/reporting/gnn/training_curves.png
  save_args:
    dpi: 300
    bbox_inches: tight
    facecolor: white

error_histograms_fig:
  type: matplotlib.MatplotlibWriter  
  filepath: data/reporting/gnn/error_histograms.png
  save_args:
    dpi: 300
    bbox_inches: tight
    facecolor: white

training_summary_fig:
  type: matplotlib.MatplotlibWriter
  filepath: data/reporting/gnn/training_summary.png
  save_args:
    dpi: 300
    bbox_inches: tight
    facecolor: white


#  ____  ____  _____   ____  ____   ___   ____ _____ ____ ____  
# |  _ \|  _ \| ____| |  _ \|  _ \ / _ \ / ___| ____/ ___/ ___| 
# | |_) | |_) |  _|   | |_) | |_) | | | | |   |  _| \___ \___ \ 
# |  __/|  _ <| |___  |  __/|  _ <| |_| | |___| |___ ___) |__) |
# |_|   |_| \_\_____| |_|   |_| \_\\___/ \____|_____|____/____/ 

# ==== 1) trace_cleaning_labelling

# -> Input
initial_raw_file:
  type: partitions.PartitionedDataset
  path: data/initial_raw
  dataset:
    type: networkanomalydetection.datasets.scapy_pcap_dataset.ScapyPcapDataset

# <- Output
trace_clean:
  type: partitions.PartitionedDataset
  path: data/pre_process/trace_clean
  dataset:
    type: networkanomalydetection.datasets.scapy_pcap_dataset.ScapyPcapDataset

trace_labels:
  type: partitions.PartitionedDataset
  path: data/pre_process/trace_labels
  dataset:
    type: pandas.CSVDataset

# ==== 2) trace_dissection

# <- Input
trace_to_dissect:
  type: partitions.PartitionedDataset
  path: data/pre_process/trace_clean
  dataset:
    type: networkanomalydetection.datasets.pyshark_pcap_dataset.PySharkPcapDataset

# <- Output
trace_dissected:
  type: partitions.PartitionedDataset
  path: data/pre_process/trace_dissection
  dataset:
    type: json.JSONDataset

# ==== 5) feature_split_type

# <- Output
feature_words:
  type: partitions.PartitionedDataset
  path: data/pre_process/vocabulary/words
  dataset:
    type: json.JSONDataset

feature_floats:
  type: partitions.PartitionedDataset
  path: data/pre_process/vocabulary/floats
  dataset:
    type: json.JSONDataset

# ==== 2) dissection_clean

# <- Output
dissected_clean:
  type: partitions.PartitionedDataset
  path: data/pre_process/dissection_clean
  dataset:
    type: json.JSONDataset

# ==== 2) dissection_clean

# <- Output
dissection_clusteried:
  type: partitions.PartitionedDataset
  path: data/pre_process/dissection_clusteried
  dataset:
    type: json.JSONDataset

# ==== 3) graph_construction

# <- Output
initial_graph:
  type: partitions.PartitionedDataset
  path: data/pre_process/graph_construction
  dataset:
    type: pickle.PickleDataset  

# ==== 4) graph_visualization

# <- Output
initial_graph_display:
  type: partitions.PartitionedDataset
  path: data/report/initial_graph_display
  dataset:
    type: text.TextDataset 

# ==== 6) feature_vectorization

# <- Output
vectorized_features:
  type: partitions.PartitionedDataset
  path: data/pre_process/feature_vectorization
  dataset:
    type: pickle.PickleDataset 

feature_vectorization_report:
  type: json.JSONDataset
  filepath: data/report/feature_vectorization.json

# ==== 7) graph_sampling

# <- Output
subgraphs:
  type: partitions.PartitionedDataset
  path: data/pre_process/graph_sampling
  dataset:
    type: pickle.PickleDataset 

sampling_report:
  type: json.JSONDataset
  filepath: data/report/sampling.json

# ==== 7) graph_conversion

# <- Output
data_loader_1:
  type: pickle.PickleDataset 
  filepath: data/pre_process/data_loaders/loader_1.pkl

data_loader_2:
  type: pickle.PickleDataset 
  filepath: data/pre_process/data_loaders/loader_2.pkl

# ==== 8) baseline_model_training
# <- Output
gnn_training_results:
  type: json.JSONDataset
  filepath: data/report/gnn_training.json